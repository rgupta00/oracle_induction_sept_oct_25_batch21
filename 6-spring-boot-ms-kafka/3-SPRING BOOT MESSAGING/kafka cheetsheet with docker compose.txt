Kafka basics:
================
Agenda:
* Intro to Kafka
* Kakfa spring boot hello world, using offset explore
* kafka consumer/producer custom objects
* Kafka Producer Example with java configuration
* Kafka error handling
* Intro to kafka streams

Kafka is distributed event driven messaging system
Millions of messages can be process per second
mutiple producer multiple broker and consumer

Kafka is distributed plateform: in production env kafk is reffered as kafka cluser 

kafka cluser : made of more then one kafka server

feature :
* kafka is fault tolerence
* in kafka cluster messages are replicated in multiple brokers
* replication factor messages is present in all the brokers
* kafka is scalable
	we can add new brokers
	we can increase no of consumers
	
kafka cluster contains many brokers and brokers is managed by zookeeper

Set up single node kafka cluster using docker compose:
---------------------------------------------------------
Step 1:
refer docker-compose.yml
docker compose up

Step 2:

Go inside container:
-----------------------
docker exec -it kafka1 bash

Create a Kafka topic using the kafka-topics command.
-----------------------------------------------------
kafka-topics --bootstrap-server kafka1:19092  --create --topic test-topic --replication-factor 1 --partitions 1

kafka-topics --bootstrap-server kafka1:19092 --list

kafka-topics --bootstrap-server kafka1:19092 --describe --topic test-topic



Produce Messages to the topic.
-------------------------------
docker exec -it kafka1 bash 
kafka-console-producer --bootstrap-server kafka1:19092 --topic test-topic


Consume Messages from the topic.
------------------------------
docker exec -it kafka1 bash 
kafka-console-consumer --bootstrap-server kafka1:19092 --topic test-topic --from-beginning





Step 3: sending and reciving messages using key-value
-----------------------------------------------------

We want message with the same key must go to the same partition


Produce Messages with Key and Value to the topic.
-------------------------------------------
docker exec -it kafka1 bash 
kafka-console-producer --bootstrap-server kafka1:19092 --topic test-topic --property "key.separator=-" --property "parse.key=true"


Consuming messages with Key and Value from a topic.
---------------------------------------------
docker exec -it kafka1 bash 
kafka-console-consumer --bootstrap-server kafka1:19092 --topic test-topic --from-beginning --property "key.separator= - " --property "print.key=true"
					   

Note: Consumer have 3 options to to read
	1. from beginning
	2. latest
	3. from a specific offset (only programmatically is possible)
	
	
List the topics in a cluster
----------------------------------
docker exec -it kafka1 bash 
kafka-topics --bootstrap-server kafka1:19092 --list



Consume Messages using Consumer Groups
-----------------------------------------
	-Consumer group is used for scalable message consumption
	-Each different app would have different consumer group
	who managage Consumer Groups?
	 Kafka broker manages Consumer Groups
	 Kafka broker act as a group coordinator
	 
list the default consumer group?
----------------------------------- 
docker exec -it kafka1 bash 
kafka-consumer-groups --bootstrap-server kafka1:19092 --list
	

Now we want to have 2 consumer listen to the producer
------------------------------------------------------
ensure you have 2 prtitions

Command to describe a specific Kafka topic.
-------------------------------------------

docker exec -it kafka1 bash 
kafka-topics --bootstrap-server kafka1:19092 --describe --topic test-topic

Alter topic Partitions
---------------------------
docker exec -it kafka1 bash 
kafka-topics --bootstrap-server kafka1:19092 --alter --topic test-topic --partitions 2

Now start 2 consumers:
--------------------
docker exec -it kafka1 bash 
kafka-console-consumer --bootstrap-server kafka1:19092 --topic test-topic --group console-consumer-15332 --property "key.separator= - " --property "print.key=true"






Commit log & retentation policies:
----------------------------------
	When kafka producer sending message, it stored in file  system as byte with extension of 00000000000.log
	these file is called partition commit logs

retentation policies?
	how long message is retained?
	configure with the property log.retention.hours in server.properties
	default is 7 days 168 hr
	
lets see where is commit log reside and where is 
retentation policies mentioned?


Log file and related config
----------------------------
docker exec -it kafka1 bash

The config file is present in the below path.
/etc/kafka/server.properties


How to view the commit log?
------------------------------
The log file is present in the below path.
/var/lib/kafka/data/

cd test-topic-1
and do ls

you will see all commit logs



Configuration of kafka as distributed streaming system?
---------------------------------------------------------
setup kafka cluster with 3 brokers
docker-compose -f docker-compose-multi-broker.yml up

setp 1: run docker compose file
	it will create cluster with 3 brokers
	
	to check the cluster
	docker ps

Create topic with the replication factor as 3
------------------------------------------
docker ps

docker exec -it kafka1 bash 
kafka-topics --bootstrap-server kafka1:19092 --create --topic test-topic --replication-factor 3 --partitions 3


list the topic:
kafka-topics --bootstrap-server kafka1:19092 --list

describe the topics 
--------------------
kafka-topics --bootstrap-server kafka1:19092 --describe


Produce Messages to the topic.
---------------------------------
docker exec -it kafka1 bash 
kafka-console-producer --bootstrap-server localhost:9092,kafka2:19093,kafka3:19094 --topic test-topic

Consume Messages from the topic.
--------------------------------
docker exec -it kafka1 bash 
kafka-console-consumer --bootstrap-server localhost:9092,kafka2:19093,kafka3:19094 --topic test-topic --from-beginning


How kafka distribute the client request?
--------------------------------------
	how topic is distributed across cluster?
	one of the broker works as controller (normally first one) one additional role to the broker
	zookeeper get command to create topic, zookeeper take care to redirect command to controller
	the role of controller is to distribute partion to different broker
	
	ie test-topic is spread over 3 brokers
	it have 3 partions
	
	partion -0 ---------> broker 1
	partion -1 ---------> broker 2
	partion -2 ---------> broker 3
	
How kafka handling data loss?
------------------------------
let assume if broker 1 is down that partion-0 data is lost, no way client have to access that data
how to solve?
we need to provide replication-factor
what is the meaning of replication-factor=3
replication-factor === no of copies of same message


In-Sync Replicas (ISR)?
------------------------
	Represent no of replicas  in synch with each other in the cluster
	it include both leaders and followers
	Recommendation value is always greater then 1
	Ideal value is ISR==Replication factor

	this can be controlled by min.insync.replicas property
		it can be set at broker level or topic level
		
		
let check the values?
-----------------------
list the topic:
kafka-topics --bootstrap-server kafka1:19092 --list

describe the topics ISR 
-----------------------------
kafka-topics --bootstrap-server kafka1:19092 --describe

let delete one of the node and see how it distribute?
---------------------------------------------------
docker ps

now describe the topics ISR 
-----------------------------
kafka-topics --bootstrap-server kafka1:19092 --describe

			
	







Spring boot kafka hello world:
--------------------------------


Step1: Spring boot producer configuration:
-------------------------------------
spring:
  kafka:
    template:
      default-topic: productapp-topic
    producer:
      bootstrap-servers: localhost:9092,localhost:9093,localhost:9094
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: org.apache.kafka.common.serialization.StringSerializer
    admin:
      properties:
        bootstrap.servers: localhost:9092,localhost:9093,localhost:9094
  application:
    name: productapp-producer
server:
  port: 8081




Step 2: configure topic programmatically
------------------------------------------

@Configuration
public class KafkaConfig {
    @Bean
    public NewTopic libraryEvents(){
        return TopicBuilder.name("productapp-topic")
                .partitions(3)
                .replicas(1)
                .build();
    }
}


step 3: create simple producer and controller 
---------------------------------------------

@Service
public class ProduceService {
	@Autowired
	private KafkaTemplate<String, String>kafkaTemplate;
	
	public void produce(String message) {
		System.out.println("message is send....");
		kafkaTemplate.send("t-hello2", message);
	}
}


@RestController
public class ProducerController {

	@Autowired
	private ProduceService produceService;
	
	@GetMapping("producer")
	public String callProducer(@RequestParam String message) {
		produceService.produce(message);
		return "ok";
	}
}


http://localhost:8080/producer?message=hello 




step 4: configuration of consumer:
-------------------------------------


application.yaml
------------------
spring:
  kafka:
    template:
      default-topic: productapp-topic
    consumer:
      bootstrap-servers: localhost:9092,localhost:9093,localhost:9094
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      group-id: productapp-events-listener-group
      auto-offset-reset: latest
  application:
    name: productapp-producer
server:
  port: 8082
  
  
configurtion consumer service:
-------------------------


@Service
public class ConsumerService {
	@KafkaListener(topics = "productapp-topic", groupId = "my_topic_group_id")
	public void consume(String message) {
		System.out.println(message);
	}
}



step 4.2: change service layer to get CompletableFuture
------------------------------------------------
@Service
public class ProductService {
	//-----------------
    public void processProduct(String message){
        CompletableFuture<SendResult<String, String>> future =
                template.send("my_topic2_sb", message);

        future.whenComplete(((result, ex) -> {
            if(ex==null){
                System.out.println(result.getRecordMetadata().offset());//:)
            }else {
                System.out.println(ex.getMessage());
            }
        }));
    }
}
step 4.3: change controller to producer 5000 messages
-----------------------------------------------------
@RestController
public class ProductController {
  
	//..........
    @GetMapping(path = "producer/{message}")
    public String processProduct(@PathVariable String message){
        for(int i=0;i<5000;i++){
            productService.processProduct(message+" "+i);
        }
        return "message is processed";
    }
}

Note: 
how to read message from a spefic partition
@KafkaListener(topics = "busycoder-demo", groupId = "my_topic_group_id", topicPartitions
		= {@TopicPartition(topic = "busycoder-demo", partitions = {"2"})})
		
		
how to send messages to the specific partition
kafkaTemplate.send("busycoder-demo", 3, null(key), data);


what if consumer want to read the massge from beginning:
-----------------------------------------------------------

spring.kafka.consumer.auto-offset-reset=earliest


step 5: kafka consumer/producer custom objects
-----------------------------------------------

step 5.1: create dto and apply annotations
-------------------------------------------
public class Product {
	private int id;
	private String name;
	private double price;
}

step 5.2: change service layer 
-------------------------------
@Service
public class ProduceService {
	
	@Autowired
	private KafkaTemplate<String, Product>kafkaTemplate;
	
	public void produce(Product product) {
		System.out.println("message is send....");
		kafkaTemplate.send("my_topic", product);
	}
}

step 5.3: change controller layer 
-------------------------------
@RestController
public class ProducerController {

	@Autowired
	private ProduceService produceService;
	@PostMapping("producer")
	public String callProducer(@RequestBody Product product) {
		produceService.produce(product);
		return "product added";
	}
}

step 5.4: change configuration
-------------------------------
server:
  port: 8080
spring:
  kafka:
    bootstrap-servers: localhost:9092
    producer:
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: org.springframework.kafka.support.serializer.JsonSerializer



Kafka consumer:
-------------------

public class Product {
	private int id;
	private String name;
	private double price;
}


@Service
public class ConsumerService {
	@KafkaListener(topics = "my_topic", groupId = "my_topic_group_id")
	public void consume(Product product) {
		System.out.println(product);
	}
}

server:
  port: 8081
spring:
  kafka:
    bootstrap-servers: localhost:9092
    consumer:
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: org.springframework.kafka.support.serializer.JsonDeserializer
      properties:
        spring.json.trusted.packages: "*"






step 6: Spring Boot with Kafka Producer Example with java configuration
--------------------------------------------------------------
import org.apache.kafka.clients.admin.NewTopic;
import org.apache.kafka.clients.producer.ProducerConfig;
import org.apache.kafka.common.serialization.StringSerializer;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.kafka.core.DefaultKafkaProducerFactory;
import org.springframework.kafka.core.KafkaTemplate;
import org.springframework.kafka.core.ProducerFactory;
import org.springframework.kafka.support.serializer.JsonSerializer;

import java.util.HashMap;
import java.util.Map;

@Configuration
public class KafkaProducerConfig {

    @Bean
    public NewTopic createTopic(){
        return new NewTopic("busycoder-demo", 3, (short) 1);
    }

    @Bean
    public Map<String,Object> producerConfig(){
        Map<String,Object> props=new HashMap<>();
        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,
                "localhost:9092");
        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,
                StringSerializer.class);
        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,
                JsonSerializer.class);
        return props;
    }

    @Bean
    public ProducerFactory<String,Object> producerFactory(){
        return new DefaultKafkaProducerFactory<>(producerConfig());
    }

    @Bean
    public KafkaTemplate<String,Object> kafkaTemplate(){
        return new KafkaTemplate<>(producerFactory());
    }

}


KafkaConsumerConfig
---------------------
import org.apache.kafka.clients.admin.NewTopic;
import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.common.serialization.StringDeserializer;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.kafka.config.ConcurrentKafkaListenerContainerFactory;
import org.springframework.kafka.config.KafkaListenerContainerFactory;
import org.springframework.kafka.core.ConsumerFactory;
import org.springframework.kafka.core.DefaultKafkaConsumerFactory;
import org.springframework.kafka.listener.ConcurrentMessageListenerContainer;
import org.springframework.kafka.support.serializer.JsonDeserializer;

import java.util.HashMap;
import java.util.Map;

@Configuration
public class KafkaConsumerConfig {

    @Bean
    public NewTopic createTopic(){
        return new NewTopic("busycoder-demo", 3, (short) 1);
    }
    @Bean
    public Map<String, Object> consumerConfig() {
        Map<String, Object> props = new HashMap<>();
        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG,
                "localhost:9092");
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG,
                StringDeserializer.class);
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,
                JsonDeserializer.class);
        props.put(JsonDeserializer.TRUSTED_PACKAGES, "*");
        return props;
    }

    @Bean
    public ConsumerFactory<String,Object> consumerFactory(){
        return new DefaultKafkaConsumerFactory<>(consumerConfig());
    }

    @Bean
    public KafkaListenerContainerFactory<ConcurrentMessageListenerContainer<String, Object>>
        kafkaListenerContainerFactory() {
        ConcurrentKafkaListenerContainerFactory<String, Object> factory =
                new ConcurrentKafkaListenerContainerFactory<>();
        factory.setConsumerFactory(consumerFactory());
        return factory;
    }
}



How to strat kafka using docker compose?
-----------------------------------------

Step 1: create docker compose file and run it

docker-compose.yml
--------------------

version: '3'

services:
  zookeeper:
    image: wurstmeister/zookeeper
    container_name: zookeeper
    ports:
      - "2181:2181"
  kafka:
    image: wurstmeister/kafka
    container_name: kafka
    ports:
      - "9092:9092"
    environment:
      KAFKA_ADVERTISED_HOST_NAME: localhost
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
	  
	  

# Command uses 
-----------------------
Move into Kafka container
docker exec -it <kafka_conatiner_id> /bin/sh

Go inside kafka installation folder
cd /opt/kafka_<version>/bin

Create Kafka topic
kafka-topics.sh --create --zookeeper zookeeper:2181 --replication-factor 1 --partitions 1 --topic quickstart

Start Producer app (CLI)
kafka-console-producer.sh --topic quickstart --bootstrap-server localhost:9092

Start consumer app (CLI)
kafka-console-consumer.sh --topic quickstart --from-beginning --bootstrap-server localhost:9092












Kafka exception handling
---------------------------

@Service
@Slf4j
public class KafkaMessageConsumer {

   	@RetryableTopic(attempts = "4", backoff =@Backoff(delay = 3000,multiplier = 1.5, maxDelay = 15000)
	,exclude = {NullPointerException.class})// 3 topic N-1
    @KafkaListener(topics = "${app.topic.name}", groupId = "busycoder-group")
    public void consumeEvents(Product p, @Header(KafkaHeaders.RECEIVED_TOPIC) String topic, 
			@Header(KafkaHeaders.OFFSET) long offset) {
		//throw exception in case of price >100000
    }

    @DltHandler
    public void listenDLT(Product p, @Header(KafkaHeaders.RECEIVED_TOPIC) String topic,
			@Header(KafkaHeaders.OFFSET) long offset) {
        log.info("DLT Received : {} , from {} , offset {}",user.getFirstName(),topic,offset);
    }
}




Example: fixed rate consumer and producer:
________________________________________
@Service
public class HelloKafkaProducer {
	@Autowired
	private KafkaTemplate<String, String>kafkaTemplate;
	
	private int i=0;
	private Logger logger=LoggerFactory.getLogger(HelloKafkaProducer.class);
	
	@Scheduled(fixedRate = 1000)
	public void sendHello() {
		i++;
		kafkaTemplate.send("t_hello", "fixed rate "+ i);
	}
}



@EnableScheduling
@SpringBootApplication
public class KafkaProducerApplication implements CommandLineRunner{
}







Kafka local setup on window machine
-------------------------------------

Download kafka:
-------------
https://archive.apache.org/dist/kafka/3.4.0/kafka_2.12-3.4.0.tgz


change: server.properties
log.dirs=c:/kafka/kafka-logs


change : zookeeper.properties
dataDir=c:/kafka/zookeeper


Kafka installation on Window:
-------------------------------


1. Start Zookeeper(port 2181)
-------------------------------
.\bin\windows\zookeeper-server-start.bat .\config\zookeeper.properties

2. Start Kafka Broker (9090)
-----------------------------
.\bin\windows\kafka-server-start.bat .\config\server.properties

3. Create topic
----------------
	Topic: communication chennal on which producer put the messages and consumer consume the the data
	for performance consideration topic divided into partitions 
	If any partition is not working we keep replication

go to window:

.\kafka-topics.bat --bootstrap-server localhost:9092 --create --topic t-hello2 --partitions 3 --replication-factor 1

List topic

.\kafka-topics.bat --bootstrap-server localhost:9092 --list

describe topic
.\kafka-topics.bat --bootstrap-server localhost:9092 --describe --topic  t-hello2

delete topic
.\kafka-topics.bat --bootstrap-server localhost:9092 --delete --topic t-hello2

4. Start Producer
--------------------
.\kafka-console-producer.bat --broker-list localhost:9092 --topic  t-hello2

Send message
How are you

5> Receive message
-------------------
.\kafka-console-consumer.bat --bootstrap-server localhost:9092 --topic  t-hello2 --from-beginning
How are you


step 1. Start Zookeeper(port 2181)
-------------------------------

.\bin\windows\zookeeper-server-start.bat .\config\zookeeper.properties

step 2. Start Kafka Broker (9090)
-----------------------------
.\bin\windows\kafka-server-start.bat .\config\server.properties



Kafka installation on Linux:
-------------------------------
Start Zookeeper
bin/zookeeper-server-start.sh config/zookeeper.properties

Start Kafka Server
bin/kafka-server-start.sh config/server.properties

Create Kafka Topic
bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic Kafka_Example

Consume from the Kafka Topic via Console
bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic Kafka_Example --from-beginning


Consumer:
# create topic t_hello
bin/kafka-topics.sh --bootstrap-server localhost:9092 --create --topic t_hello --partitions 1 --replication-factor 1

# list topic
bin/kafka-topics.sh --bootstrap-server localhost:9092 --list

# describe topic
bin/kafka-topics.sh --bootstrap-server localhost:9092 --describe --topic t_hello

# create topic t_test
bin/kafka-topics.sh --bootstrap-server localhost:9092 --create --topic t_test --partitions 1 --replication-factor 1

# delete topic t_test
bin/kafka-topics.sh --bootstrap-server localhost:9092 --delete --topic t_test

https://www.youtube.com/playlist?list=PLOcnwq0FwVrQd7fxM64p6aR4mIxG3ZdYw